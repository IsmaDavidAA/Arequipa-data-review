{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989b05c9",
   "metadata": {},
   "source": [
    "# Data extraction and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e30b9a",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b291dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "import shutil\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "from shapely.geometry import shape, Point\n",
    "from geopy.distance import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468c765",
   "metadata": {},
   "source": [
    "### Get the paths to the folders in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf71b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_folder = os.getcwd()\n",
    "root_project = os.path.abspath(os.path.join(notebook_folder, '.'))\n",
    "dataset_logs = os.path.abspath(os.path.join(root_project, 'Datos', 'Logs'))\n",
    "trufi_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Registros de Trufi App'))\n",
    "municipios_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Poligonos','peru_provincial_simple.geojson'))\n",
    "csv_file_path = os.path.join(trufi_datos, 'origin-destination.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab4a7c",
   "metadata": {},
   "source": [
    "### Set the time zone of Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1bd0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case Arequipa, Peru\n",
    "analysis_timezone = pytz.timezone('America/Lima')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192de2a9",
   "metadata": {},
   "source": [
    "### Extract logs requests to origin-destination.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0542a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routes matching the pattern have been saved to D:\\TrufiData\\Datos\\Registros de Trufi App\\origin-destination.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_route_info(log_line):\n",
    "    # Regular expression to extract specific information from route requests\n",
    "    route_pattern_with_id = re.compile(r'GET /otp/routers/default/plan\\?fromPlace=([-0-9.]+)%2C([-0-9.]+)&toPlace=([-0-9.]+)%2C([-0-9.]+).*?Trufi/.*?/([a-f0-9-]+)')\n",
    "\n",
    "    match_with_id = route_pattern_with_id.search(log_line)\n",
    "    # Initialize variables with default values\n",
    "    origin_latitude = origin_longitude = dest_latitude = dest_longitude = id_user = None\n",
    "    \n",
    "    try:\n",
    "        if match_with_id:\n",
    "            origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user = match_with_id.groups()\n",
    "        return origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing line: {log_line}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def process_log_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'GET /otp/routers/default/plan' in line:\n",
    "                date_str = re.search(r'\\[([^:]+:[^ ]+)', line).group(1)\n",
    "                # Convert the date to a datetime object and add the La Paz timezone\n",
    "                date_time = datetime.strptime(date_str, '%d/%b/%Y:%H:%M:%S').replace(tzinfo=pytz.utc).astimezone(analysis_timezone)\n",
    "                route_info = extract_route_info(line)\n",
    "                if route_info:\n",
    "                    origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user = route_info\n",
    "                    yield [date_time.strftime('%Y-%m-%d %H:%M:%S'), origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user]\n",
    "                    \n",
    "# Get the list of files and sort them by numeric prefix\n",
    "file_pattern = re.compile(r'^\\d{2}-')\n",
    "files = os.listdir(dataset_logs)\n",
    "log_files = [file for file in files if file_pattern.match(file)]\n",
    "log_files.sort(key=lambda x: int(x.split('-')[0]))\n",
    "\n",
    "header = ['date', 'origin_latitude', 'origin_longitude', 'destination_latitude', 'destination_longitude', 'userID']\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(header)\n",
    "    for log_file in log_files:\n",
    "        file_path = os.path.join(dataset_logs, log_file)\n",
    "        \n",
    "        route_info_generator = process_log_file(file_path)\n",
    "        \n",
    "        # Write the lines to the CSV file\n",
    "        csv_writer.writerows(route_info_generator)\n",
    "\n",
    "print(f\"Routes matching the pattern have been saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281fa43",
   "metadata": {},
   "source": [
    "### Discard requests without user ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843ba744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1441 rows without userID from 23892.\n"
     ]
    }
   ],
   "source": [
    "def filter_requests_with_userid(csv_file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "    # Count rows before filtering\n",
    "    total_rows_before = len(df)\n",
    "\n",
    "    # Filter the requests that have userID\n",
    "    df_filtered = df[df['userID'].notnull()]\n",
    "\n",
    "    # Count rows after filtering\n",
    "    total_rows_after = len(df_filtered)\n",
    "\n",
    "    # Calculate how many rows were removed\n",
    "    rows_removed = total_rows_before - total_rows_after\n",
    "\n",
    "    # Overwrite the original file with the filtered requests\n",
    "    df_filtered.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    return total_rows_before, rows_removed\n",
    "\n",
    "# Call the function to filter the requests\n",
    "total_rows_before, rows_removed = filter_requests_with_userid(csv_file_path)\n",
    "\n",
    "# Print the number of removed rows\n",
    "print(f\"Removed {rows_removed} rows without userID from {total_rows_before}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff29ec51",
   "metadata": {},
   "source": [
    "### Number of users to within the evaluated date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e7e267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique users is: 4408\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "# Get the number of unique users\n",
    "unique_users_count = df['userID'].nunique()\n",
    "\n",
    "# Print the number of unique users\n",
    "print(f\"The number of unique users is: {unique_users_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f09e2f",
   "metadata": {},
   "source": [
    "### Generate syntetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e067bf24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     date  origin_latitude  origin_longitude  \\\n",
      "0     2024-09-03 23:28:18       -16.375103        -71.557241   \n",
      "1     2024-09-03 23:28:38       -16.429629        -71.529506   \n",
      "2     2024-09-03 23:29:13       -16.375103        -71.557241   \n",
      "3     2024-09-03 23:29:21       -16.429629        -71.529506   \n",
      "4     2024-09-03 23:30:52       -16.375103        -71.557241   \n",
      "...                   ...              ...               ...   \n",
      "22446 2024-09-30 20:20:59       -16.436882        -71.530523   \n",
      "22447 2024-09-30 20:23:45       -16.452287        -71.525863   \n",
      "22448 2024-09-30 20:25:41       -16.420176        -71.500390   \n",
      "22449 2024-09-30 20:25:57       -16.420176        -71.500390   \n",
      "22450 2024-09-30 20:32:33       -16.407343        -71.539034   \n",
      "\n",
      "       destination_latitude  destination_longitude  \\\n",
      "0                -16.429629             -71.529506   \n",
      "1                -16.375103             -71.557241   \n",
      "2                -16.429629             -71.529506   \n",
      "3                -16.375103             -71.557241   \n",
      "4                -16.429629             -71.529506   \n",
      "...                     ...                    ...   \n",
      "22446            -16.376728             -71.560333   \n",
      "22447            -16.373087             -71.559432   \n",
      "22448            -16.419797             -71.499902   \n",
      "22449            -16.423180             -71.525967   \n",
      "22450            -16.406868             -71.491356   \n",
      "\n",
      "                                     userID  hour  day_of_week  day_of_month  \\\n",
      "0      58d343f1-70f5-418c-930f-b3ff4b5db4da    23            1             3   \n",
      "1      58d343f1-70f5-418c-930f-b3ff4b5db4da    23            1             3   \n",
      "2      58d343f1-70f5-418c-930f-b3ff4b5db4da    23            1             3   \n",
      "3      58d343f1-70f5-418c-930f-b3ff4b5db4da    23            1             3   \n",
      "4      58d343f1-70f5-418c-930f-b3ff4b5db4da    23            1             3   \n",
      "...                                     ...   ...          ...           ...   \n",
      "22446  f26ecc02-c976-448f-9ada-65e64ec277a1    20            0            30   \n",
      "22447  0d8f88e2-1313-4081-96dc-f1c65eca411e    20            0            30   \n",
      "22448  f791a138-0eb3-4523-9bba-184590c55051    20            0            30   \n",
      "22449  f791a138-0eb3-4523-9bba-184590c55051    20            0            30   \n",
      "22450  adb4c48f-d51b-4063-b8a6-cdd4c228eac5    20            0            30   \n",
      "\n",
      "       weekend year_week_number time_of_day  \n",
      "0            0           202435       night  \n",
      "1            0           202435       night  \n",
      "2            0           202435       night  \n",
      "3            0           202435       night  \n",
      "4            0           202435       night  \n",
      "...        ...              ...         ...  \n",
      "22446        0           202439       night  \n",
      "22447        0           202439       night  \n",
      "22448        0           202439       night  \n",
      "22449        0           202439       night  \n",
      "22450        0           202439       night  \n",
      "\n",
      "[22451 rows x 12 columns]\n",
      "The filtered requests have been saved to: D:\\TrufiData\\Datos\\Registros de Trufi App\\origin-destination.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file with time and distance features\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'], encoding='latin1')\n",
    "\n",
    "# Add time and distance features\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['day_of_week'] = df['date'].dt.dayofweek  # Monday: 0, Sunday: 6\n",
    "df['day_of_month'] = df['date'].dt.day\n",
    "df['weekend'] = (df['date'].dt.weekday >= 5).astype(int)  # 1 if weekend, 0 if not\n",
    "# Add variable for the week number of the year\n",
    "df['year_week_number'] = df['date'].dt.strftime('%Y%U')\n",
    "\n",
    "# Function to assign the time of day period\n",
    "def assign_time_of_day(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "# Apply the function to create the new variable 'time_of_day'\n",
    "df['time_of_day'] = df['hour'].apply(assign_time_of_day)\n",
    "\n",
    "# Show the resulting DataFrame with the new variable\n",
    "print(df)\n",
    "\n",
    "# Save the filtered DataFrame with the new variable to a new CSV file\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"The filtered requests have been saved to: {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968ff8f",
   "metadata": {},
   "source": [
    "### Exclude requests for routes with a distance of less than 300 meters between origin and destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edda5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "# Create points for the latitude and longitude columns\n",
    "df['origin_point'] = df.apply(lambda row: Point(row['origin_longitude'], row['origin_latitude']), axis=1)\n",
    "df['destination_point'] = df.apply(lambda row: Point(row['destination_longitude'], row['destination_latitude']), axis=1)\n",
    "\n",
    "# Calculate the distance between origin and destination in meters\n",
    "df['distance'] = df.apply(lambda row: distance((row['origin_latitude'], row['origin_longitude']),\n",
    "                                               (row['destination_latitude'], row['destination_longitude'])).meters, axis=1)\n",
    "\n",
    "# Filter the DataFrame to keep only the points that have a distance greater than 300 meters\n",
    "df_filtered = df[df['distance'] > 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331997c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests before filtering: 22451\n",
      "Requests after filtering: 21026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idaas\\AppData\\Local\\Temp\\ipykernel_16272\\3715189283.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.drop(['origin_point', 'destination_point'], axis=1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered requests have been saved to: D:\\TrufiData\\Datos\\Registros de Trufi App\\origin-destination.csv\n"
     ]
    }
   ],
   "source": [
    "# Count rows before filtering\n",
    "rows_before = len(df)\n",
    "\n",
    "# Count rows after filtering\n",
    "rows_after = len(df_filtered)\n",
    "\n",
    "# Print the number of rows before and after filtering\n",
    "print(f\"Requests before filtering: {rows_before}\")\n",
    "print(f\"Requests after filtering: {rows_after}\")\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file without the origin_point and destination_point columns\n",
    "df_filtered.drop(['origin_point', 'destination_point'], axis=1, inplace=True)\n",
    "df_filtered.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"The filtered requests have been saved to: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85afdc8d",
   "metadata": {},
   "source": [
    "### Generate variables for origin and destination municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7ef70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "# Add the 'origin_municipio' and 'dest_municipio' columns with default values\n",
    "df['origin_municipio'] = 'external'\n",
    "df['dest_municipio'] = 'external'\n",
    "\n",
    "# Save the updated DataFrame to the CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03dda4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoDataFrame from the GeoJSON\n",
    "arequipa_gdf = gpd.read_file(municipios_datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42548375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\TrufiData\\\\Datos\\\\Registros de Trufi App\\\\origin-destination.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the get_city_from_coords function to return NOMBPROV\n",
    "def get_city_from_coords(coords):\n",
    "    point = Point(coords)\n",
    "    # Iterate over the features of the filtered Arequipa GeoJSON\n",
    "    for index, row in arequipa_gdf.iterrows():\n",
    "        city_geometry = row['geometry']\n",
    "        # Check if the point is within the city's geometry\n",
    "        if point.within(city_geometry):\n",
    "            return row['NOMBPROV']  # Return the NOMBPROV instead of FIRST_NOMB\n",
    "    \n",
    "    return 'external'\n",
    "\n",
    "# Add the columns 'origin_municipio' and 'dest_municipio' to the DataFrame\n",
    "columns_to_add = ['origin_municipio', 'dest_municipio']\n",
    "\n",
    "# Create an empty DataFrame with the new columns\n",
    "df = pd.DataFrame(columns=columns_to_add)\n",
    "# Open the CSV file for reading and read the existing columns\n",
    "with open(csv_file_path, 'r') as csvfile:\n",
    "    # Read the first line to get the column names\n",
    "    existing_columns = csvfile.readline().strip().split(',')\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame(columns=existing_columns)\n",
    "\n",
    "    # Create a temporary file to write the updated lines\n",
    "    with open('temp_csvfile.csv', 'w', newline='') as temp_csvfile:\n",
    "        # Write the column names to the temporary file\n",
    "        temp_csvfile.write(','.join(existing_columns) + '\\n')\n",
    "\n",
    "        for i, line in enumerate(csvfile):\n",
    "            # Read a line from the CSV file\n",
    "            row = pd.read_csv(io.StringIO(line), header=None, names=existing_columns).iloc[0]\n",
    "\n",
    "            # Get municipality names for origin and destination\n",
    "            origin_municipio = get_city_from_coords((row['origin_longitude'], row['origin_latitude']))\n",
    "            dest_municipio = get_city_from_coords((row['destination_longitude'], row['destination_latitude']))\n",
    "\n",
    "            # Add the values to the DataFrame\n",
    "            row['origin_municipio'] = origin_municipio\n",
    "            row['dest_municipio'] = dest_municipio\n",
    "\n",
    "            # Write the updated line to the temporary file\n",
    "            temp_csvfile.write(','.join(map(str, row.values)) + '\\n')\n",
    "shutil.move('temp_csvfile.csv', csv_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
